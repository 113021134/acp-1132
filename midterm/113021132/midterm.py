# -*- coding: utf-8 -*-
"""MidTerm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tuVA4VCfS95d56J8kcFfC0AXWXu9lo5t
"""

!pip install scrapy

!scrapy startproject githubscraper
!cd githubscraper

import os
os.chdir('/content/githubscraper')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile githubscraper/spiders/github_spider.py
# import scrapy
# import requests
# import re
# 
# class GithubSpider(scrapy.Spider):
#     name = "github_spider"
#     start_urls = ["https://github.com/nolandoalvin?tab=repositories"]
# 
#     def parse(self, response):
#         repos = response.css('li[itemprop="owns"]')
#         for repo in repos:
#             repo_url = response.urljoin(repo.css('a[itemprop="name codeRepository"]::attr(href)').get())
#             repo_name = repo.css('a[itemprop="name codeRepository"]::text').get().strip()
#             about = repo.css('p[itemprop="description"]::text').get()
#             about = about.strip() if about else None
#             last_updated = repo.css('relative-time::attr(datetime)').get()
# 
#             yield scrapy.Request(
#                 repo_url,
#                 callback=self.parse_repo,
#                 meta={
#                     'repo_url': repo_url,
#                     'repo_name': repo_name,
#                     'about': about,
#                     'last_updated': last_updated
#                 }
#             )
# 
#     def parse_repo(self, response):
#         repo_url = response.meta['repo_url']
#         repo_name = response.meta['repo_name']
#         about = response.meta['about']
#         last_updated = response.meta['last_updated']
# 
#         is_empty = response.css('div.Blankslate').get() is not None
#         if not about and not is_empty:
#             about = repo_name
# 
#         languages = response.css('ul.list-style-none li span[itemprop="programmingLanguage"]::text').getall()
#         languages = ', '.join([lang.strip() for lang in languages]) if languages else None
# 
#         commits = None
#         if not is_empty:
#             api_url = f"https://api.github.com/repos/nolandoalvin/{repo_name}/commits?per_page=1"
#             try:
#                 r = requests.get(api_url)
#                 if r.status_code == 200:
#                     link_header = r.headers.get('Link', '')
#                     if 'rel="last"' in link_header:
#                         match = re.search(r'&page=(\d+)>; rel="last"', link_header)
#                         if match:
#                             commits = int(match.group(1))
#                     else:
#                         commits = 1
#             except Exception as e:
#                 self.logger.error(f"Error getting commits for {repo_name}: {e}")
#                 commits = None
# 
#         if is_empty:
#             languages = None
#             commits = None
# 
#         yield {
#             'repo_name': repo_name,
#             'repo_url': repo_url,
#             'about': about,
#             'last_updated': last_updated,
#             'languages': languages,
#             'number_of_commits': commits
#         }

!scrapy crawl github_spider -o output.xml