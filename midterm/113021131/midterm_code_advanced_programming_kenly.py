# -*- coding: utf-8 -*-
"""Midterm code advanced programming Kenly.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e_47S8SbQHeuVFJwjcdp4sssMkgVtNoJ
"""

!pip install scrapy

import scrapy
from scrapy.crawler import CrawlerProcess

class GithubRepoSpider(scrapy.Spider):
    name = 'github'

    def start_requests(self):
        yield scrapy.Request(url='https://github.com/113021131?tab=repositories', callback=self.parse)

    def parse(self, response):
        repos = response.css('li[itemprop="owns"]')
        for repo in repos:
            relative_url = repo.css('a[itemprop="name codeRepository"]::attr(href)').get()
            full_url = response.urljoin(relative_url)
            about = repo.css('p[itemprop="description"]::text').get()
            last_updated = repo.css('relative-time::attr(datetime)').get()

            yield response.follow(full_url, callback=self.parse_repo_detail, meta={
                'url': full_url,
                'about': about.strip() if about else None,
                'last_updated': last_updated
            })

    def parse_repo_detail(self, response):
        item = {
            'url': response.meta['url'],
            'about': response.meta['about'] or response.url.split('/')[-1],
            'last_updated': response.meta['last_updated']
        }


        empty = response.css('div.Box-body p::text').re_first(r'This repository is empty')
        if empty:
            item['languages'] = None
            item['commits'] = None
        else:

            langs = response.css('li.d-inline span.color-fg-default.text-bold.mr-1::text').getall()
            item['languages'] = [lang.strip() for lang in langs if lang.strip()] or None


            commits_text = response.css('span.fgColor-default::text').get()
            item['commits'] = commits_text if commits_text else None

        yield item


from scrapy.utils.project import get_project_settings
from google.colab import files

process = CrawlerProcess(settings={
    "FEEDS": {"output.xml": {"format": "xml"}},
    "LOG_LEVEL": "ERROR",
    "DOWNLOAD_DELAY": 1,
    "USER_AGENT": "Mozilla/5.0 (X11; Linux x86_64)"
})


process.crawl(GithubRepoSpider)
process.start()


with open("output.xml", "r") as f:
    print(f.read())


files.download("output.xml")